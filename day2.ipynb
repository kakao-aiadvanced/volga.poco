{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q7QAoYB-sVhO"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVlXnB36saI9",
        "outputId": "406a11f8-e384-4c69-8a2c-85bd84ed220c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX5KZX7gtLFx",
        "outputId": "f23b37a6-f9e4-4a2a-9326-c6219e5488aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "\n",
        "example_messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfLCVfLXtLIm",
        "outputId": "87164afb-ac0a-443d-a0cf-6e4ca3cda10b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "rN0B8MQ2tLLk",
        "outputId": "e84c826d-e260-4549-8658-5817423ba289"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task decomposition is the process of breaking down a complex task into smaller, manageable steps. This can be achieved using techniques like Chain of Thought (CoT) and Tree of Thoughts, which guide models to think step by step and explore multiple reasoning paths. These methods enhance performance by allowing for structured problem-solving and clearer interpretations of the model's reasoning process.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 1-10 task 에 대한 코드를, 각각의 task별 참고 링크를 보면서 작성해보세요."
      ],
      "metadata": {
        "id": "NA_M0ljgvPkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 3개의 블로그 포스팅 본문을 Load하기 : WebBaseLoader 활용**\n",
        "\n",
        "https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/"
      ],
      "metadata": {
        "id": "DE2fz9WuvRAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=urls,\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(f\"로드된 문서 수: {len(docs)}\")\n",
        "for i, doc in enumerate(docs):\n",
        "    print(f\"\\n문서 {i+1}:\")\n",
        "    print(f\"출처: {doc.metadata['source']}\")\n",
        "    print(f\"내용 미리보기: {doc.page_content[:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhTTW94mtcIP",
        "outputId": "3bf7781f-c1c5-4975-abdc-3f26ad399071"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "로드된 문서 수: 3\n",
            "\n",
            "문서 1:\n",
            "출처: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "내용 미리보기: \n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |...\n",
            "\n",
            "문서 2:\n",
            "출처: https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\n",
            "내용 미리보기: \n",
            "\n",
            "      Prompt Engineering\n",
            "    \n",
            "Date: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: ...\n",
            "\n",
            "문서 3:\n",
            "출처: https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\n",
            "내용 미리보기: \n",
            "\n",
            "      Adversarial Attacks on LLMs\n",
            "    \n",
            "Date: October 25, 2023  |  Estimated Reading Time: 33 min  ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 불러온 본문을 Split (Chunking) : recursive text splitter 활용 (아래 링크 참고)**\n",
        "\n",
        "https://python.langchain.com/v0.2/docs/how_to/recursive_text_splitter/"
      ],
      "metadata": {
        "id": "bxIJAd8Ovm9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # 각 청크의 최대 크기 (문자 수)\n",
        "    chunk_overlap=200,  # 청크 간 겹침 크기 (문자 수)\n",
        "    length_function=len,  # 크기 측정 함수\n",
        "    is_separator_regex=False,  # 구분자를 정규식으로 해석하지 않음\n",
        ")\n",
        "\n",
        "# 2. 문서 분할\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 3. 결과 확인\n",
        "print(f\"분할된 청크 수: {len(splits)}\")\n",
        "for i, split in enumerate(splits[:3]):  # 처음 3개 청크만 확인\n",
        "    print(f\"\\n청크 {i+1}:\")\n",
        "    print(f\"출처: {split.metadata['source']}\")\n",
        "    print(f\"내용 미리보기: {split.page_content[:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w8tQpbYtcQ5",
        "outputId": "e149be14-44f0-48ad-c7ef-d2082414f8ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "분할된 청크 수: 183\n",
            "\n",
            "청크 1:\n",
            "출처: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "내용 미리보기: LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
            "\n",
            "청크 2:\n",
            "출처: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "내용 미리보기: Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as ...\n",
            "\n",
            "청크 3:\n",
            "출처: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "내용 미리보기: Fig. 1. Overview of a LLM-powered autonomous agent system.\n",
            "Component One: Planning#\n",
            "A complicated ta...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Chunks 를 임베딩하여 Vector store 저장: openai 임베딩, chroma vectorstore 사용**\n",
        "\n",
        "이 때,\n",
        "\n",
        "embedding model 은 \"text-embedding-3-small\" 사용\n",
        "\n",
        "embedding: https://python.langchain.com/v0.2/docs/integrations/text_embedding/openai/\n",
        "\n",
        "vetor store: https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/\n",
        "\n",
        "retriever search_type 은 'similarity', search_kwargs={'k': 6} 을 사용해주세요."
      ],
      "metadata": {
        "id": "Rh9-K_fSwFeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "J4K6SOzFtcUJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"my_collection\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_langchain_db\"  # 로컬 저장 경로\n",
        ")"
      ],
      "metadata": {
        "id": "0akUEI3ewx2j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(splits))]\n",
        "\n",
        "# 벡터 스토어에 청크 추가\n",
        "vector_store.add_documents(documents=splits, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXw5iy0cw7mS",
        "outputId": "6aaa29dc-dd21-40ea-e7d1-aad5b8bca1e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a78cf58d-3f28-47c3-85c9-8ef78a4930ad',\n",
              " '61aebec9-2f0f-4d35-836c-69548af066b2',\n",
              " 'e0ae7744-d32b-4e25-a402-e5f4719dfc06',\n",
              " '40c5222f-fb22-4d66-b4aa-d29a6e0e811b',\n",
              " '9a5c7694-f8d3-46d5-8e71-beb3bb53ed2a',\n",
              " '9c474788-17d4-40d5-a6de-84a7d5253860',\n",
              " 'fb9dabe3-eeaa-4e16-9c3e-87065129b71a',\n",
              " 'b85b629a-f113-4f2a-82e1-9d075e186839',\n",
              " '3fbd879a-8037-4fdb-916f-c09e3616358a',\n",
              " 'e35a87ec-7cfd-4e36-ad57-c93dbee72e13',\n",
              " 'f1acfab5-4e7f-48be-9373-6b41e1d1c434',\n",
              " '54ec9c31-bfe7-40bc-803e-df636a3e6f94',\n",
              " 'bf853be9-023b-4712-9273-ee9e5b44c112',\n",
              " '54056b02-62bd-4fac-8ed7-cf7e89c2cebe',\n",
              " 'ace81352-d31f-46ef-9a08-5580d05f8ea1',\n",
              " 'a4b849ae-033b-499f-93b2-1d81f1154055',\n",
              " 'e4c8cd0a-1359-49f1-ace8-a4afd7535d9c',\n",
              " 'd486524d-f690-4822-afdf-14254bbb8bc4',\n",
              " 'f6e59a06-e4a3-41f7-9f82-7931324efff2',\n",
              " 'ed4502ee-711e-45a4-9299-07bff4c85776',\n",
              " '5e7fec5f-3338-4c85-bc17-26d372afb57a',\n",
              " '98581423-9088-472f-bfd6-4c5b126b2599',\n",
              " '1333a955-b04c-4872-9c08-35e83e786148',\n",
              " '6f3dc5a2-d23f-4eaf-bb85-21e051fab4e3',\n",
              " 'a4fb1e55-1bc4-4952-8e5c-159e44c41165',\n",
              " '0a12f786-ddf4-4faf-b4b6-8f540c065b0f',\n",
              " 'b1480812-d581-4148-bd4d-05f87e5fd89e',\n",
              " '63b69cdf-718c-4329-b8ce-ac42e1020dcd',\n",
              " '8c5b3231-5ca6-419d-8953-fd28ac150075',\n",
              " 'dec7bd44-5a81-4023-9a33-d8744f776652',\n",
              " '54d727ee-df80-487d-be67-3e43c28fede2',\n",
              " '3bab0f56-6157-48c5-a172-1d1aaf59038b',\n",
              " 'b6313470-06d2-4ca0-83e4-9188659226e3',\n",
              " '3e10da00-7c07-444f-8003-7cd0350d37d0',\n",
              " 'e820d6be-ae25-4472-92b4-8f2d75df68b4',\n",
              " '9534b6cc-6743-4302-ab16-2fa9fe4f5e93',\n",
              " 'b3f6aaa4-78ec-463b-a1ce-558a5bd132cf',\n",
              " '258abc99-d86f-4bcf-9c38-9b5f29dda519',\n",
              " 'f31d224c-d1c3-4a2f-9814-bd7b24a89b78',\n",
              " '78e3e2ce-452d-4e7e-814b-fba5897281ed',\n",
              " '775853f8-3382-4052-adae-41b82cfaccf9',\n",
              " 'fcd8d8fa-feb4-43f4-b712-bac05587a79d',\n",
              " '9cc8daba-b77c-4551-9a32-92eabb4246c4',\n",
              " '0a965a19-df6c-4dac-ba9a-d86c166f60b8',\n",
              " 'a4e9c523-1637-4fd5-9da0-8064c0f21b3b',\n",
              " '90931d91-2e23-4400-93e1-7d8fb9a9ad65',\n",
              " 'eac35df8-ee47-44c9-ad75-23311bab45f6',\n",
              " '14ab9351-db9a-4f9d-8c3c-b536cb9fabb2',\n",
              " 'ebac21e3-ad96-4cfc-aa35-3347c3185daf',\n",
              " '2e371c43-529d-45d5-b3d8-434c2191afb0',\n",
              " '24bcaabb-aab6-448f-9223-0e8cde77ca0e',\n",
              " '271d6df4-af73-4b8d-83d1-9f31f3e55d2b',\n",
              " '91055b69-e41f-449f-9ebb-0ae742742641',\n",
              " 'b98955b9-c3ad-4b1c-b028-17a30c81df87',\n",
              " '090098dd-fe70-4aa4-97c0-9c23c4fa3800',\n",
              " 'f3334edc-5c0b-49c1-a597-1068d512e66a',\n",
              " '20ec2330-d609-4678-a7ca-14d3e5df14df',\n",
              " '0cc36abe-5d1a-4676-9d1f-15ff37ece150',\n",
              " 'd2749bb0-911a-4070-a50b-7dd0983ace4e',\n",
              " '7a56ecbb-3b3b-4920-9105-0db453e96546',\n",
              " 'bad4f406-e8ba-4a07-ad42-e1b0804c724a',\n",
              " '64592ae5-2f06-47e3-8e51-9572a75d9988',\n",
              " '0db4fab1-30e9-45e5-bed9-d4b464c738d2',\n",
              " '4e3e105b-bea7-45ed-a080-0aab88fc7b1e',\n",
              " 'ba66a78d-687f-4c23-ab92-977830290688',\n",
              " 'bd892d88-71b4-4153-be64-512c3b4bb7ca',\n",
              " 'c69800d4-9e56-4cfb-8c75-27c40124c46f',\n",
              " '5cfd97dc-ed18-4d03-8648-1a721e153be0',\n",
              " '3d8d60f9-3896-4eaa-b7e6-5d698ad69f18',\n",
              " '0cb9b28e-6c89-4d84-8327-9eb5e6f83284',\n",
              " '3ffe7b15-11d4-4070-85c8-295d16b23825',\n",
              " 'e1e86621-aa6e-4369-9b3e-af59f49313c2',\n",
              " 'a453027a-c21d-432f-ac13-ef53fe0607e9',\n",
              " '60c39327-aada-4507-bb03-6fb1688c28f5',\n",
              " 'b3f1ad82-b69a-4914-b07b-6600ec1c507b',\n",
              " 'ab7f43e9-ca11-4d82-88bf-34db7511884b',\n",
              " '6af6c550-8f6e-4e79-a734-fb988f720222',\n",
              " '6d9af778-8bef-4626-ad99-f9de9087f9ad',\n",
              " 'e9c6958c-de21-4cc7-a840-970140dee2eb',\n",
              " '233865a6-1a73-4862-b679-4ecd3dd47216',\n",
              " '40d81c9a-c960-4ef6-b22c-fd04c5d66add',\n",
              " 'cb7bc5a8-1a43-454e-959d-721b099f5da4',\n",
              " '2bd04420-a882-4c0f-ac95-94ee33598166',\n",
              " '5fc9ad9b-466f-468c-9040-7ed4bc2e6522',\n",
              " 'eb3f5636-3160-4a36-89eb-da931ee7c30c',\n",
              " '58b89ec9-e4c0-41f8-b35b-17535688605c',\n",
              " 'ea66e972-9c85-4556-9d84-9849e2b40992',\n",
              " '01655c4b-5f9f-46ad-a63f-eec17c99c4b5',\n",
              " '9ed8550d-af1a-4dbe-915e-81ef851cbb60',\n",
              " '97e0b402-a235-49d1-8668-2bfca547653b',\n",
              " '6911a05e-c77b-4e44-b726-1969a5ef324c',\n",
              " 'df08f465-32c0-40f5-86e2-b8ba1a756d26',\n",
              " 'e447e5e4-e7d7-4237-b967-e75c15309f2f',\n",
              " '9809482b-80ac-435a-9d08-1173f9c75257',\n",
              " '3204fc52-1ef8-4996-9ed6-160415c309a9',\n",
              " '2e5dd508-894f-457d-a070-efcd513f4d30',\n",
              " 'e290ca84-fb89-442d-84a9-4a58d9713dbb',\n",
              " 'e77308c2-a4fe-456b-86a0-536e98f9d803',\n",
              " 'aabcfb83-3c49-4992-9046-358aeefd41f3',\n",
              " 'e262d6af-469a-4fdd-b66d-33f964ade5f2',\n",
              " 'fd90ead0-4cfa-4c30-b1d0-df15110da2ec',\n",
              " '1742ef78-ede1-42f0-b19b-cceff9b113a3',\n",
              " '144a8b5b-bb28-4dc9-bf6b-c05d733000ae',\n",
              " '480f6883-1491-4e56-8c7b-6a21709c52a0',\n",
              " 'dcfb0bf4-78e5-4715-a5fd-bd3290312cdb',\n",
              " '998df938-000a-4caf-a5f7-afc6d4d0ed83',\n",
              " 'f253ef71-f6e6-4dbd-9ce7-a996c1746f60',\n",
              " 'e9e63038-4e2f-4d6c-adcd-a143179f2fd8',\n",
              " 'c32a45e0-514a-4df4-9318-deb1a96f834a',\n",
              " '19e8fcca-39aa-48c3-a69c-ccf4c34de1d2',\n",
              " '31303c41-61f7-4b6d-b75b-69a459b7fe99',\n",
              " '4d0a9136-9892-4758-a0e0-efe9ae2f7a50',\n",
              " '67dc16b4-0fc4-4161-94ef-ce223b7d28ef',\n",
              " '1421945f-475f-4cd8-a926-faa2227fb4ba',\n",
              " '93f39bed-3ede-42df-9086-a6504d5252e9',\n",
              " 'bebf2730-196b-4c51-9fe8-1cabc57146be',\n",
              " 'da940374-2aef-47c1-a55f-c11ef49f4fe9',\n",
              " '4f643529-2ccb-4928-b738-163ea6ca7a44',\n",
              " '96009228-2ae2-4bfd-883e-776870475de5',\n",
              " 'c5f81f29-b158-4235-9906-4d0182ce2d11',\n",
              " '6bdf2323-16cc-4dd3-b765-a661be1ebdd3',\n",
              " '34da6758-436a-4069-840a-2e6519f10695',\n",
              " '6349dcad-bc0b-4515-b390-a0d3ba29ecee',\n",
              " 'cf372e1d-8676-4661-8bf5-fce9593510a7',\n",
              " '924acdc1-eab0-42e8-97b3-179f4d695ac2',\n",
              " '0542e804-fcaa-41c2-b65d-5a11ae5e8b14',\n",
              " 'c2a84460-b494-4d18-b968-68ba090f846c',\n",
              " 'ef33e8f4-f88d-43f0-a381-ea02cd35c4d8',\n",
              " '7216f215-9f1d-4d64-9643-7bf1b0d18618',\n",
              " 'e63fc84a-5fab-4b0e-9747-97c77f08049d',\n",
              " 'dcdb0a08-2bf9-4493-9047-85662ae8b518',\n",
              " '89fa7359-4848-4372-a163-9379dd60375c',\n",
              " 'fcd4099a-0a56-4f49-9d48-72fb9768db0e',\n",
              " '846d6a8c-9003-4f88-8df9-026f12aa537c',\n",
              " '2df433a9-74e1-40f6-be43-88eba261714a',\n",
              " '1ca7e988-37fd-4b27-a525-36f5f973878c',\n",
              " '3d66e5e9-adb0-4417-90f1-b4b7ce203dec',\n",
              " '3a855421-8cc4-4a71-ac7c-2c8bbf1263f5',\n",
              " 'debb1463-ca7f-4b52-aa1e-a4bc8575df29',\n",
              " 'd95e343d-e29d-4c9f-8b92-0e9145158fa7',\n",
              " 'cd4ddc9e-07b6-48d7-9811-28bbb44e31d8',\n",
              " '149aedd3-c094-4c3d-bcd4-d5d945d03cb1',\n",
              " '5c6cad3c-f320-40be-9965-399595541552',\n",
              " '2a848d12-844c-46c9-adc0-83d4f3bd40cd',\n",
              " '9e7f8a44-baca-4495-b681-477514f23511',\n",
              " '70eb2050-3689-42dd-a549-ca78c8c2478b',\n",
              " '78f0bf68-2961-4d8d-a3f8-c8c232d7f827',\n",
              " 'bacddd8c-0f71-4d22-bd2f-2e9a9e94baad',\n",
              " 'b8b08da4-f806-48b6-91f7-6fae6ce60743',\n",
              " 'c140d59f-d70f-4c17-a9ff-58c80fd130b1',\n",
              " '356513f0-4a41-453c-901c-aba84bcadf20',\n",
              " '813dde72-f7fe-4005-9ae5-6dbe41dc06a2',\n",
              " 'd04ee707-c538-41bc-9de8-1435441807eb',\n",
              " 'd23d5982-9f3e-4735-8d03-bcb7f216c1af',\n",
              " '8310e429-c013-47e4-8e01-25e8018743ce',\n",
              " 'e054e339-bb14-4482-8d8f-352fb0100f58',\n",
              " '4c3770b7-2d3a-45d5-bb58-0d374fffebab',\n",
              " 'e96bccdc-8697-4243-a2a4-243a26a6244f',\n",
              " 'dd9409a8-f55e-46d8-bb15-fef0a70d7a8a',\n",
              " '33145775-593a-46c9-b051-26365b995e2a',\n",
              " '30a35ade-65c9-45a8-b8fe-a15b1b0baaac',\n",
              " '7b663ee6-f605-4204-aa2e-a1bab4e19e37',\n",
              " 'a2559f63-246b-4b26-a12d-3b0f55961405',\n",
              " '3127b9ca-1346-40af-a763-03d1027b5a36',\n",
              " '3aef834a-ce13-41dd-9a36-b8651bd493d7',\n",
              " 'c8d495fd-7642-419b-b4ba-e67b9bc64e44',\n",
              " 'cf037f85-bfce-498c-9b37-1b277cc4aafb',\n",
              " '00d56aae-484d-4463-badc-6b6ff62bb7c8',\n",
              " 'aff9ff21-cf9b-4cce-ab56-11480a45e87f',\n",
              " '7b1adcbb-c5be-4a50-9147-6ae22a0db2ab',\n",
              " '932069c3-cade-416c-8a0d-5d2f5721a0e5',\n",
              " '1bd77ee6-7497-41d0-a335-1a50df21868a',\n",
              " '46ed2861-f5c9-45fd-8af2-1d2d47fce2e6',\n",
              " 'cd3add0a-e064-432a-b1a7-8833c21903ab',\n",
              " '722bd94f-6bcf-4686-9a28-787f5606b778',\n",
              " 'a6f2bde1-1288-4df5-b1d5-d495b59c101d',\n",
              " '0e5dfc16-d169-4caf-9d09-79f56dc11bcf',\n",
              " '9aaf0683-5c9a-4e28-8143-102bcab52c76',\n",
              " '0c5d41ed-42fb-4771-ab81-0eb048d4bead',\n",
              " 'e705838b-1437-434f-b92f-9df06d7814b3',\n",
              " '0ae63251-a7a1-4776-91d3-912394dee970',\n",
              " '1b8247ed-04ab-4584-bddf-5a0e01730ada',\n",
              " '229f4147-a5b5-4dc9-b018-16a7fc401953']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 6}\n",
        ")"
      ],
      "metadata": {
        "id": "DspmcuYixDbn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. User query = ‘agent memory’ 를 받아 관련된 chunks를 retrieve**\n",
        "\n",
        "\n",
        "https://python.langchain.com/v0.2/docs/how_to/vectorstore_retriever/\n",
        "\n",
        "https://api.python.langchain.com/en/latest/vectorstores/langchain_core.vectorstores.VectorStore.html#langchain_core.vectorstores.VectorStore.as_retriever"
      ],
      "metadata": {
        "id": "Vrk84nJ3xKFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"agent memory\"\n",
        "retrieved_docs = retriever.invoke(query)"
      ],
      "metadata": {
        "id": "9zMSmi2gxqGT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"Document {i+1}: {doc.page_content[:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSIhu3acxuvs",
        "outputId": "cf03dce7-2be3-4ecd-933e-2be30402f9f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 6 documents:\n",
            "Document 1: Memory stream: is a long-term memory module (external database) that records a comprehensive list of...\n",
            "Document 2: LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
            "Document 3: Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as ...\n",
            "Document 4: Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\n",
            "This fun simulation res...\n",
            "Document 5: Reliability of natural language interface: Current agent system relies on natural language as an int...\n",
            "Document 6: They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test s...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. 5-1) 과 5-2) 를 참고하여, User query와 retrieved chunk 에 대해 relevance 가 있는지를 평가하는 시스템 프롬프트를 작성해보세요: retrieval 퀄리티를 LLM 이 스스로 평가하도록 하고, 관련이 있으면 {‘relevance’: ‘yes’} 관련이 없으면 {‘relevance’: ‘no’} 라고 출력하도록 함. ( JsonOutputParser() 를 활용 )**\n"
      ],
      "metadata": {
        "id": "_g6x-NJqyJHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-1 참고) RAG 용 프롬프트 작성을 위한 Prompt Hub 활용\n",
        "\n",
        "https://smith.langchain.com/hub/rlm/rag-prompt"
      ],
      "metadata": {
        "id": "whpJh8xsyLz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "prompt"
      ],
      "metadata": {
        "id": "5fyVEUhAxuoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "\n",
        "example_messages"
      ],
      "metadata": {
        "id": "fyjD37gUynIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-2 참고) output JSON formatting 하는 코드\n",
        "\n",
        "https://python.langchain.com/v0.2/docs/how_to/output_parser_json/#without-pydantic"
      ],
      "metadata": {
        "id": "AqERzmHMyn55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-2 참고 코드\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "joke_query = \"Tell me a joke.\"\n",
        "\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "chain.invoke({\"query\": joke_query})"
      ],
      "metadata": {
        "id": "ktiXqEe5ywYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cmtCLgZ1tJ3",
        "outputId": "001eb2cb-09ca-41a9-fa8a-0b6cc9a9d524"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.10)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.48)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.39)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.68.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "client = Client(api_key=getpass.getpass())\n",
        "prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkMvK-5k1eUo",
        "outputId": "0b6b8e5c-4826-4174-e6c6-bf7670a1c854"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 모듈 임포트\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 1. LLM 초기화 (예: OpenAI GPT 모델 사용)\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# 2. JsonOutputParser 설정\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "# 3. 프롬프트 템플릿 정의\n",
        "prompt_template = \"\"\"\n",
        "You are an AI assistant tasked with evaluating the relevance of a retrieved document chunk to a user's query.\n",
        "\n",
        "User query: {query}\n",
        "\n",
        "Retrieved chunk: {chunk}\n",
        "\n",
        "Determine if the retrieved chunk is relevant to the user's query. Output your decision in JSON format as follows:\n",
        "- If relevant: {{\"relevance\": \"yes\"}}\n",
        "- If not relevant: {{\"relevance\": \"no\"}}\n",
        "\"\"\"\n",
        "\n",
        "# 4. PromptTemplate 생성\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"query\", \"chunk\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "# 5. 체인 구성\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# 6. 체인 실행 예시\n",
        "user_query = \"agent memory\"\n",
        "retrieved_chunk = \"Memory is a crucial component of an agent, allowing it to retain information over time.\"\n",
        "\n",
        "result = chain.invoke({\"query\": user_query, \"chunk\": retrieved_chunk})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPl0Z66Z0Psz",
        "outputId": "bf65ef6c-b9fd-461a-9cf4-c5190937e902"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'relevance': 'yes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. 5 에서 모든 docs에 대해 'yes' 가 나와야 하는 케이스와 ‘no’ 가 나와야 하는 케이스를 작성해보세요."
      ],
      "metadata": {
        "id": "ggVRJI5W05v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"agent memory\"\n",
        "\n",
        "chunks = [\n",
        "    \"Memory is a crucial component of an agent, allowing it to retain information over time.\",\n",
        "    \"In reinforcement learning, agents use memory to store past experiences, which helps in decision-making.\",\n",
        "    \"Agent memory can be short-term or long-term, depending on the task requirements.\"\n",
        "]\n",
        "\n",
        "for i, chunk in enumerate(chunks, start=1):\n",
        "    result = chain.invoke({\"query\": user_query, \"chunk\": chunk})\n",
        "    print(f\"Result {i}: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG9_ugMk2SjM",
        "outputId": "85ab144b-0d93-46a3-f23c-e7fa8db95e59"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1: {'relevance': 'yes'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 2: {'relevance': 'yes'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 3: {'relevance': 'yes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"agent memory\"\n",
        "\n",
        "chunks = [\n",
        "    \"The weather forecast for tomorrow is sunny and warm, with a high of 82 degrees.\",\n",
        "    \"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    \"The top 10 soccer players in the world right now.\"\n",
        "]\n",
        "\n",
        "for i, chunk in enumerate(chunks, start=1):\n",
        "    result = chain.invoke({\"query\": user_query, \"chunk\": chunk})\n",
        "    print(f\"Result {i}: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcnM1s4x2SXD",
        "outputId": "05b56c69-0f38-47e5-8322-34c25ebd7113"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1: {'relevance': 'no'}\n",
            "Result 2: {'relevance': 'no'}\n",
            "Result 3: {'relevance': 'no'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. 6-7 의 평가에서 문제가 없다면, 5에서 작성한 코드의 실행 결과가 'yes' 인 경우, 4의 retrieved chunk 를 가지고 답변하는 chain 코드를 작성해주세요. (prompt | llm | parser 형태의 코드)"
      ],
      "metadata": {
        "id": "v3nrUYKr3vb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI  # LLM 예시로 OpenAI 사용\n",
        "\n",
        "# 1. 필요한 함수 정의\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "relevance_prompt = ChatPromptTemplate.from_template(\n",
        "    \"주어진 쿼리 '{query}'와 청크 '{chunk}'를 보고 청크가 쿼리와 관련이 있는지 'yes' 또는 'no'로 답하세요.\"\n",
        ")\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # LLM 초기화\n",
        "relevance_chain = relevance_prompt | llm | StrOutputParser()\n",
        "\n",
        "retrieved_docs = retriever.invoke(query)\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"Document {i+1}: {doc.page_content[:100]}...\")\n",
        "\n",
        "relevant_chunks = []\n",
        "for doc in retrieved_docs:\n",
        "    relevance_result = relevance_chain.invoke({\"query\": query, \"chunk\": doc.page_content})\n",
        "    if relevance_result.strip().lower() == \"yes\":  # 결과가 'yes'인지 확인\n",
        "        relevant_chunks.append(doc)\n",
        "\n",
        "if relevant_chunks:\n",
        "    print(\"\\nRelevance confirmed. Generating answer...\")\n",
        "    # RAG 체인 구성 (context로 relevant_chunks 사용)\n",
        "    rag_chain = (\n",
        "        {\"context\": lambda x: format_docs(relevant_chunks), \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    # 스트리밍으로 답변 출력\n",
        "    for chunk in rag_chain.stream(query):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "else:\n",
        "    print(\"\\nNo relevant chunks found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEPtVdnO3vDK",
        "outputId": "1d5becf5-b109-4dee-c4f0-6f4dae61f771"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-374a207c11b0>:14: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # LLM 초기화\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 6 documents:\n",
            "Document 1: Memory stream: is a long-term memory module (external database) that records a comprehensive list of...\n",
            "Document 2: LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
            "Document 3: Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as ...\n",
            "Document 4: Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\n",
            "This fun simulation res...\n",
            "Document 5: Reliability of natural language interface: Current agent system relies on natural language as an int...\n",
            "Document 6: They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Relevance confirmed. Generating answer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent memory consists of both short-term and long-term memory modules. Short-term memory utilizes in-context learning, while long-term memory retains and recalls information over extended periods, often using an external database. This memory system allows agents to synthesize past experiences and"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " inform future behavior."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. 생성된 답안에 Hallucination 이 있는지 평가하는 시스템 프롬프트를 작성해보세요. LLM이 스스로 평가하도록 하고, hallucination 이 있으면 {‘hallucination’: ‘yes’} 없으면 {‘hallucination’: ‘no’} 라고 출력하도록 하세요."
      ],
      "metadata": {
        "id": "QAgMWZBpywHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 1. LLM 초기화\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# 2. JSON 파서 설정\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "# 3. 수정된 프롬프트 템플릿 정의\n",
        "prompt_template = \"\"\"\n",
        "You are an AI assistant tasked with evaluating whether a generated answer contains hallucination. Hallucination occurs when the answer includes information that is not factually correct or not supported by evidence.\n",
        "\n",
        "Generated answer: {answer}\n",
        "\n",
        "Determine if the generated answer contains hallucination. Output your decision in JSON format as follows:\n",
        "- If there is hallucination: {{\"hallucination\": \"yes\"}}\n",
        "- If there is no hallucination: {{\"hallucination\": \"no\"}}\n",
        "\"\"\"\n",
        "\n",
        "# 4. PromptTemplate 생성\n",
        "prompt = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"answer\"]\n",
        ")\n",
        "\n",
        "# 5. 체인 구성\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "# 6. 체인 실행 예시\n",
        "generated_answer = \"The capital of France is Paris, and it is known for its beautiful beaches.\"\n",
        "result = chain.invoke({\"answer\": generated_answer})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXrE4Pg66NWq",
        "outputId": "21692692-dca7-4638-ffec-d573bea231d4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hallucination': 'yes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. 9 에서 ‘yes’ 면 8 로 돌아가서 다시 생성, ‘no’ 면 답변 생성하고 유저에게 답변 생성에 사용된 출처와 함께 출력하도록 하세요. (최대 1번까지 다시 생성)"
      ],
      "metadata": {
        "id": "6KM-Fb3JsaZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 가정: Task 8에서 답변을 생성하는 함수\n",
        "def generate_answer(query, retrieved_docs):\n",
        "    # RAG 프롬프트와 LLM 설정\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")  # LangChain Hub에서 RAG 프롬프트 가져오기\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")  # LLM 모델 설정\n",
        "    rag_chain = (\n",
        "        {\"context\": lambda x: \"\\n\".join([doc.page_content for doc in retrieved_docs]),\n",
        "         \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return rag_chain.invoke(query)\n",
        "\n",
        "# 가정: Task 9에서 Hallucination을 평가하는 함수\n",
        "def evaluate_hallucination(answer):\n",
        "    hallucination_prompt = PromptTemplate(\n",
        "        template=\"\"\"\n",
        "        You are an AI assistant tasked with evaluating whether a generated answer contains hallucination.\n",
        "        Generated answer: {answer}\n",
        "        Output your decision in JSON format: {{\"hallucination\": \"yes\"}} or {{\"hallucination\": \"no\"}}\n",
        "        \"\"\",\n",
        "        input_variables=[\"answer\"]\n",
        "    )\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    parser = JsonOutputParser()\n",
        "    hallucination_chain = hallucination_prompt | llm | parser\n",
        "    return hallucination_chain.invoke({\"answer\": answer})\n",
        "\n",
        "# Task 10: 답변 생성 및 평가 로직\n",
        "def task_10(query, retrieved_docs, max_attempts=2):\n",
        "    attempt = 0\n",
        "    while attempt < max_attempts:\n",
        "        # 1. 답변 생성\n",
        "        answer = generate_answer(query, retrieved_docs)\n",
        "        # 2. Hallucination 평가\n",
        "        evaluation = evaluate_hallucination(answer)\n",
        "\n",
        "        if evaluation.get(\"hallucination\") == \"no\":\n",
        "            # 출처 추출 (retrieved_docs의 metadata에서 'source' 가져오기)\n",
        "            sources = [doc.metadata.get(\"source\", \"Unknown\") for doc in retrieved_docs]\n",
        "            return answer, sources\n",
        "\n",
        "        # Hallucination이 'yes'이면 재시도\n",
        "        attempt += 1\n",
        "\n",
        "    # 최대 시도 횟수 초과 시 실패 메시지 반환\n",
        "    return \"최대 재시도 횟수를 초과하여 hallucination 없는 답변을 생성하지 못했습니다.\", []\n",
        "\n",
        "# 실행 예시\n",
        "query = \"What is Task Decomposition?\"\n",
        "retrieved_docs = retriever.invoke(query)  # Task 4에서 정의된 retriever 사용 가정\n",
        "answer, sources = task_10(query, retrieved_docs)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Answer:\", answer)\n",
        "print(\"Sources:\", sources)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkjq4N_-69I4",
        "outputId": "7b619a16-adbf-480d-c896-4ad274011dd2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Task decomposition is the process of breaking down a complex task into smaller, manageable sub-tasks. This can be done using techniques like prompting language models to think step by step or through human input for specific tasks. It allows the model to tackle complicated problems systematically, enhancing performance and clarity in thought processes.\n",
            "Sources: ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://lilianweng.github.io/posts/2023-06-23-agent/']\n"
          ]
        }
      ]
    }
  ]
}